#De-Kayne 2023

#####################################################################################
#LOGIN

#Kamiak
ssh rishi.de-kayne@kamiak.wsu.edu

#Hummingbird
ssh rdekayne@hb.ucsc.edu

#####################################################################################
#IMPORTANT DIRECTORIES from kamiak

#Raw data collation
#sample fastq 
cd /data/kelley/projects/poeciliids_2022/

#data
cd /data/kelley/projects/kerry/pmex_evo_rates_aa_convergence

#genomes
/data/kelley/projects/kerry/reference_genomes

#####################################################################################
#move fastq files to hummingbird - log into hummingbird and run
scp -r rishi.de-kayne@kamiak.wsu.edu:/data/kelley/projects/poeciliids_2022/* .
scp -r rishi.de-kayne@kamiak.wsu.edu:/data/kelley/projects/anthonys_projects/concatenated_whole_genome_data/sga_preqc/*.gz .
scp -r rishi.de-kayne@kamiak.wsu.edu:/data/kelley/projects/kerry/reference_genomes/GCF_001443325.1_P_mexicana* .

#################
#     01 MULTIQC
#################
# 01_H2S_QC.sh 

#!/bin/bash
#SBATCH --job-name=fastqQC
#SBATCH --time=0-12:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --partition=128x24
#SBATCH --output=QC.out # output file
#SBATCH --error=QC.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=2 # 2 CPUs per job
#SBATCH --mem=4GB # Memory limit of 4GB

module load hb hb-gnu fastq

cd /hb/home/rdekayne/01_H2S_QC
ls ../data/*.fastq.gz > samples.txt
sed -i 's/\.\.\/data\///g' samples.txt
sed -i 's/_R1_001\.fastq\.gz//g' samples.txt
grep -v "_R2_" samples.txt > sample_list.txt
rm samples.txt

cat sample_list.txt | while read sample
do
fastp -i /hb/home/rdekayne/data/${sample}_R1_001.fastq.gz -I /hb/home/rdekayne/data/${sample}_R2_001.fastq.gz -o /hb/home/rdekayne/01_H2S_QC/output_QC/${sample}_R1.out.fastq.gz -O /hb/home/rdekayne/01_H2S_QC/output_QC/${sample}_R2.out.fastq.gz -h ${sample}.html && touch /hb/home/rdekayne/01_H2S_QC/output_QC/${sample}.done
done

touch all_samples.done


#################
#     02 REFERENCES
#################
#get references
#hellerii - https://www.ncbi.nlm.nih.gov/genome/15325
#guppy - https://www.ncbi.nlm.nih.gov/genome/23338

#index 
#module load hb hb-gnu bwa-mem2/bwa-mem2-2.2.1
#/hb/software/apps/bwa-mem2/gnu-2.2.1/bin/bwa-mem2

# 02_genome_indexing.sh

#!/bin/bash
#SBATCH --job-name=genome_index
#SBATCH --time=0-4:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --partition=128x24
#SBATCH --output=index.out # output file
#SBATCH --error=index.err # error file
#SBATCH --ntasks=1 # Run 1 job
#SBATCH --ntasks-per-node=1 # One task per computer
#SBATCH --cpus-per-task=1 # 2 CPUs per job
#SBATCH --mem=64GB 

module load hb hb-gnu bwa-mem2/bwa-mem2-2.2.1

cd /hb/home/rdekayne/ref_genomes
/hb/software/apps/bwa-mem2/gnu-2.2.1/bin/bwa-mem2 index ./GCF_000633615.1_Guppy_female_1.0_MT_genomic.fna
/hb/software/apps/bwa-mem2/gnu-2.2.1/bin/bwa-mem2 index ./GCF_003331165.1_Xiphophorus_hellerii-4.1_genomic.fna
touch ./indices.done

#calculate assembly stats
#https://github.com/KorfLab/Assemblathon/blob/master/assemblathon_stats.pl
#http://korflab.ucdavis.edu/Unix_and_Perl/FAlite.pm

# 02_genome_stats.sh

#!/bin/bash
#SBATCH --job-name=genome_stats
#SBATCH --time=0-4:00:00 # Wall clock time limit in Days-Hours:min:seconds
#SBATCH --partition=128x24
#SBATCH --output=stats.out # output file
#SBATCH --error=stats.err # error file
#SBATCH --ntasks=1 
#SBATCH --ntasks-per-node=1 
#SBATCH --cpus-per-task=1 
#SBATCH --mem=8GB 

perl -I /hb/home/rdekayne/ref_genomes/ assemblathon_stats.pl ./GCF_000633615.1_Guppy_female_1.0_MT_genomic.fna
perl -I /hb/home/rdekayne/ref_genomes/ assemblathon_stats.pl ./GCF_003331165.1_Xiphophorus_hellerii-4.1_genomic.fna

touch stats.done

#################
#     03 MAPPING
#################
# 03.1_H2S_mapping.sh

#!/bin/bash
## load modules 
module load hb hb-gnu bwakit/bwa-0.7.15

module load gcc/4.8.2 gdc bwa/0.7.17 java/1.8.0_73 sambamba/0.6.8
##number of processors used per job
proc=20
##Defining all the paths 
reference="reference.fasta"
input="/path/to/seqprep/output"
out="/path/to/output"

#name of the individual that will be mapped (Filename must start with these Characters)
ind=$LSB_JOBINDEX
name="DF$ind"
short=$name

#BWA:
#forward and reverse:
bwa mem -t ${proc} -r 1 -M ${reference} ${input}/${name}_R1.polyg.n.fastq.gz ${input}/${name}_R2.polyg.n.fastq.gz > ${TMPDIR}/${short}_FR_n.sam
sambamba view -S ${TMPDIR}/${short}_FR_n.sam -f bam -o ${out}/${short}_FR_n.bam -t ${proc} -l 9


#02.2_H2S_processing.sh
#!/bin/bash


## load modules 
module load gcc/4.8.2 gdc samtools/1.9 java/1.8.0_73 picard-tools/2.20.2 sambamba/0.6.8

#information for each individual, respectively each bam file
ind=$LSB_JOBINDEX
short="DF$ind"
rg=$(echo "0$LSB_JOBINDEX")
library=$( echo "library$short")
lane=$( echo "1.$rg")

###Defining all the paths 
input="/path/to/input/bam"
output="/path/to/output/directory"
proc=1
##====================================================================================================
#Forward and reverse

picard FixMateInformation I=${input}/${short}_FR_n.bam O=/dev/stdout VALIDATION_STRINGENCY=LENIENT | picard AddOrReplaceReadGroups I=/dev/stdin O=${TMPDIR}/${short}_int1_n.bam RGID=${lane} RGlb=${library} RGPL=illumina RGSM=${rg} RGPU=unit1  
sambamba sort ${TMPDIR}/${short}_int1_n.bam -o ${TMPDIR}/${short}_FR_sorted_n.bam -t ${proc} -m 50GB --tmpdir ${TMPDIR}
picard MarkDuplicates TMP_DIR=${TMPDIR} INPUT=${TMPDIR}/${short}_FR_sorted_n.bam OUTPUT=${output}/${short}_FR_sorted_dup_WFRef_nova.bam METRICS_FILE=${output}/${short}_FR_duplication_metrics_WFRef.txt VALIDATION_STRINGENCY=LENIENT MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=1024
sambamba index ${output}/${short}_FR_sorted_dup_WFRef_nova.bam
rm ${TMPDIR}/*

#Merged 
picard FixMateInformation I=${input}/${short}_M_n.bam O=/dev/stdout VALIDATION_STRINGENCY=LENIENT | picard AddOrReplaceReadGroups I=/dev/stdin O=${TMPDIR}/${short}_M_read_group_n.bam RGID=${lane} RGlb=${library} RGPL=illumina RGSM=${rg} RGPU=unit1 VALIDATION_STRINGENCY=LENIENT
sambamba sort ${TMPDIR}/${short}_M_read_group_n.bam -o ${TMPDIR}/${short}_M_sorted_n.bam -t ${proc} -m 50GB --tmpdir ${TMPDIR}
picard MarkDuplicates TMP_DIR=${TMPDIR} INPUT=${TMPDIR}/${short}_M_sorted_n.bam OUTPUT=${output}/${short}_M_sorted_dup_WFRef_nova.bam METRICS_FILE=${output}/${short}_M_duplication_metrics_WFRef.txt VALIDATION_STRINGENCY=LENIENT MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=1024
sambamba index ${output}/${short}_M_sorted_dup_WFRef_nova.bam

#################
#     03 GENOTYPING + FILTERING
#################
cat /data/martin/genomics/analyses/Danaus_mapping/DC174/coverage_raw/scaf.list | while read line
do
echo "bcftools mpileup -O u -d 250 --skip-indels -f /data/martin/genomics/analyses/Danaus_genome/Dchry2/Dchry2.2.fa --annotate FORMAT/DP --bam-list /scratch/rdekayne/genotyping/bam.list -r "${line}" | bcftools call -m -f GQ -O v | bgzip > /scratch/rdekayne/genotyping/"${line}".realigned.raw.vcf.gz && touch /data/martin/genomics/analyses/Danaus_popgen/DC174/genotyping/done_files/"${line}".realigned.raw.vcf.gz.done" >> geno_realigned_1pop.txt
done

parallel -j 1 'qsub -cwd -N bcftools -V -pe smp64 1 -l h=biggar -b yes {}' :::: geno_realigned_1pop.txt

touch make.filt.chroms.txt
cat /data/martin/genomics/analyses/Danaus_popgen/DC174/raw_chr_vcfs/vcf.list | while read line
do
echo "bcftools filter -Oz /scratch/rdekayne/genotyping/"${line}" -e 'FORMAT/DP < 7 | FORMAT/GQ < 30' --set-GTs . -O u | bcftools filter -e 'AN < 2' | bcftools sort -Oz -o /data/martin/genomics/analyses/Danaus_popgen/DC174/filt_chr_vcfs/"${line}"_filt_mindepth7_minqual30.vcf.gz" >> make.filt.chroms.txt
done

parallel -j 1 'qsub -cwd -N bcftools -V -pe smp64 1 -l h=biggar -b yes {}' :::: make.filt.chroms.txt
